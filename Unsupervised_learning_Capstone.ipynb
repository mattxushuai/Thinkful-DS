{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unit 4 Unsupervised Learning Capstone Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project you'll dig into a large amount of text and apply most of what you've covered in this unit and in the course so far.\n",
    "\n",
    "First, pick a set of texts. This can be either a series of novels, chapters, or articles. Anything you'd like. It just has to have multiple entries of varying characteristics. At least 100 should be good. There should also be at least 10 different authors, but try to keep the texts related (either all on the same topic of from the same branch of literature - something to make classification a bit more difficult than obviously different subjects).\n",
    "\n",
    "This capstone can be an extension of your NLP challenge if you wish to use the same corpus. If you found problems with that data set that limited your analysis, however, it may be worth using what you learned to choose a new corpus. Reserve 25% of your corpus as a test set.\n",
    "\n",
    "The first technique is to create a series of clusters. Try several techniques and pick the one you think best represents your data. Make sure there is a narrative and reasoning around why you have chosen the given clusters. Are authors consistently grouped into the same cluster?\n",
    "\n",
    "Next, perform some unsupervised feature generation and selection using the techniques covered in this unit and elsewhere in the course. Using those features then build models to attempt to classify your texts by author. Try different permutations of unsupervised and supervised techniques to see which combinations have the best performance.\n",
    "\n",
    "Lastly return to your holdout group. Does your clustering on those members perform as you'd expect? Have your clusters remained stable or changed dramatically? What about your model? Is it's performance consistent?\n",
    "\n",
    "If there is a divergence in the relative stability of your model and your clusters, delve into why.\n",
    "\n",
    "Your end result should be a write up of how clustering and modeling compare for classifying your texts. What are the advantages of each? Why would you want to use one over the other? Approximately 3-5 pages is a good length for your write up, and remember to include visuals to help tell your story!\n",
    "\n",
    "Dataset: https://www.kaggle.com/snap/amazon-fine-food-reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\shuaix\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Basic imports \n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "#NLP imports \n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#Dimension Reduction \n",
    "from sklearn.decomposition import TruncatedSVD, PCA\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "#Clustering \n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.cluster import MeanShift, estimate_bandwidth\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "\n",
    "#Clustering Evaluation \n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "#Model Imports  \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "\n",
    "#Model Optimization \n",
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.model_selection import RandomizedSearchCV \n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#Time\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://www.dropbox.com/s/d4ye48a67tth2ae/Reviews.csv?dl=1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Score                                               Text\n",
       "0      5  I have bought several of the Vitality canned d...\n",
       "1      1  Product arrived labeled as Jumbo Salted Peanut...\n",
       "2      4  This is a confection that has been around a fe...\n",
       "3      2  If you are looking for the secret ingredient i...\n",
       "4      5  Great taffy at a great price.  There was a wid..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#drop uneccessary columns \n",
    "df.drop(['Id','ProductId','UserId','ProfileName','HelpfulnessNumerator','HelpfulnessDenominator','Time', 'Summary'],axis=1,inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 568454 entries, 0 to 568453\n",
      "Data columns (total 2 columns):\n",
      "Score    568454 non-null int64\n",
      "Text     568454 non-null object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 8.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Score    0\n",
       "Text     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicate reviews  \n",
    "df.drop_duplicates(subset=['Score','Text'],keep='first',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    250745\n",
       "4     56074\n",
       "1     36280\n",
       "3     29772\n",
       "2     20804\n",
       "Name: Score, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Score.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to clean text.\n",
    "def text_cleaner(text):\n",
    "    \n",
    "    # Visual inspection shows spaCy does not recognize the double dash '--'.\n",
    "    \n",
    "    text = re.sub(r'--',' ',text)\n",
    "    \n",
    "    # Removes hyperlinks \n",
    "    text = re.sub(r'<a\\s+href=(?:\"([^\"]+)\"|\\'([^\\']+)\\').*?>(.*?)</a>',' ', text)\n",
    "    \n",
    "    # Get rid of extra whitespace.\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    # Lowercase text\n",
    "    text = text.lower()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Clean'] = df['Text'].apply(lambda x: text_cleaner(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Utility function to lemmatize our text reviews limiting variations on same words\n",
    "lemma = spacy.lang.en.English()\n",
    "\n",
    "def lemma_text(text):\n",
    "    tokens = lemma(text) \n",
    "    return([token.lemma_ for token in tokens if not token.is_punct and not token.is_stop])\n",
    "\n",
    "df['lemma_text'] = df.Clean.apply(lemma_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "      <th>Text</th>\n",
       "      <th>Clean</th>\n",
       "      <th>lemma_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "      <td>i have bought several of the vitality canned d...</td>\n",
       "      <td>[buy, vitality, can, dog, food, product, find,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "      <td>product arrived labeled as jumbo salted peanut...</td>\n",
       "      <td>[product, arrive, label, jumbo, salt, peanut, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "      <td>this is a confection that has been around a fe...</td>\n",
       "      <td>[confection, century, light, pillowy, citrus, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "      <td>if you are looking for the secret ingredient i...</td>\n",
       "      <td>[look, secret, ingredient, robitussin, believe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "      <td>great taffy at a great price. there was a wide...</td>\n",
       "      <td>[great, taffy, great, price, wide, assortment,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Score                                               Text  \\\n",
       "0      5  I have bought several of the Vitality canned d...   \n",
       "1      1  Product arrived labeled as Jumbo Salted Peanut...   \n",
       "2      4  This is a confection that has been around a fe...   \n",
       "3      2  If you are looking for the secret ingredient i...   \n",
       "4      5  Great taffy at a great price.  There was a wid...   \n",
       "\n",
       "                                               Clean  \\\n",
       "0  i have bought several of the vitality canned d...   \n",
       "1  product arrived labeled as jumbo salted peanut...   \n",
       "2  this is a confection that has been around a fe...   \n",
       "3  if you are looking for the secret ingredient i...   \n",
       "4  great taffy at a great price. there was a wide...   \n",
       "\n",
       "                                          lemma_text  \n",
       "0  [buy, vitality, can, dog, food, product, find,...  \n",
       "1  [product, arrive, label, jumbo, salt, peanut, ...  \n",
       "2  [confection, century, light, pillowy, citrus, ...  \n",
       "3  [look, secret, ingredient, robitussin, believe...  \n",
       "4  [great, taffy, great, price, wide, assortment,...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying variables\n",
    "X = df['Clean']\n",
    "y = df['Score']\n",
    "\n",
    "# Splitting into train and test sets, reserve 40% for test \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.40, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total word count \n",
    "feature_df = pd.DataFrame()\n",
    "feature_df['word_count'] = [len(x.split()) for x in X_train.tolist()]\n",
    "\n",
    "# Count of punctuations \n",
    "feature_df['exclamation_marks'] = X_train.str.findall(r'[!]').str.len()\n",
    "feature_df['periods'] = X_train.str.findall(r'[.]').str.len()\n",
    "feature_df['question_marks'] = X_train.str.findall(r'[?]').str.len()\n",
    "feature_df['Text'] = X_train\n",
    "feature_df['Score'] = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_count</th>\n",
       "      <th>exclamation_marks</th>\n",
       "      <th>periods</th>\n",
       "      <th>question_marks</th>\n",
       "      <th>Text</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>i have bought several of the vitality canned d...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>204</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>product arrived labeled as jumbo salted peanut...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>175</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>this is a confection that has been around a fe...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>86</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>if you are looking for the secret ingredient i...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>great taffy at a great price. there was a wide...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   word_count  exclamation_marks  periods  question_marks  \\\n",
       "0          64                0.0      3.0             0.0   \n",
       "1         204                0.0      5.0             0.0   \n",
       "2         175                0.0      9.0             0.0   \n",
       "3          86                0.0      3.0             0.0   \n",
       "4          60                0.0      4.0             0.0   \n",
       "\n",
       "                                                Text  Score  \n",
       "0  i have bought several of the vitality canned d...    5.0  \n",
       "1  product arrived labeled as jumbo salted peanut...    1.0  \n",
       "2  this is a confection that has been around a fe...    4.0  \n",
       "3  if you are looking for the secret ingredient i...    2.0  \n",
       "4  great taffy at a great price. there was a wide...    5.0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's check our new features\n",
    "feature_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 69650\n"
     ]
    }
   ],
   "source": [
    "# Initialize vectorizer \n",
    "vectorizer = TfidfVectorizer(max_df=0.5, # drop words that occur in more than half the paragraphs\n",
    "                             min_df=2, # only use words that appear at least twice\n",
    "                             use_idf=True,#we definitely want to use inverse document frequencies in our weighting\n",
    "                             norm=u'l2', #Applies a correction factor so that longer paragraphs and shorter paragraphs get treated equally\n",
    "                             smooth_idf=True, #Adds 1 to all document frequencies, as if an extra document existed that used every word once.  Prevents divide-by-zero errors\n",
    "                             tokenizer=lemma_text)\n",
    "\n",
    "\n",
    "# Applying the vectorizer\n",
    "X_tfidf = vectorizer.fit_transform(X)\n",
    "print(\"Number of features: %d\" % X_tfidf.get_shape()[1])\n",
    "\n",
    "# Splitting into train and test sets\n",
    "X_train_tfidf, X_test_tfidf, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.40, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent variance captured by all components: 54.44782775510504\n"
     ]
    }
   ],
   "source": [
    "#Our SVD data reducer.  We are going to reduce the feature space from upwards of 70k to 1500 \n",
    "#Ideally we would try to capture no less than 90% variance in the dataset, but due to computational limitations, we'll have to sacrifice variance\n",
    "svd= TruncatedSVD(800)\n",
    "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "# Run SVD on the training data, then project the training data.\n",
    "X_train_lsa = lsa.fit_transform(X_train_tfidf)\n",
    "\n",
    "variance_explained=svd.explained_variance_ratio_\n",
    "total_variance = variance_explained.sum()\n",
    "\n",
    "print(\"Percent variance captured by all components:\",total_variance*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>790</th>\n",
       "      <th>791</th>\n",
       "      <th>792</th>\n",
       "      <th>793</th>\n",
       "      <th>794</th>\n",
       "      <th>795</th>\n",
       "      <th>796</th>\n",
       "      <th>797</th>\n",
       "      <th>798</th>\n",
       "      <th>799</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.138526</td>\n",
       "      <td>-0.019096</td>\n",
       "      <td>-0.009700</td>\n",
       "      <td>-0.091483</td>\n",
       "      <td>-0.041705</td>\n",
       "      <td>0.043097</td>\n",
       "      <td>-0.143737</td>\n",
       "      <td>0.060551</td>\n",
       "      <td>-0.029851</td>\n",
       "      <td>0.122997</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000889</td>\n",
       "      <td>0.003325</td>\n",
       "      <td>0.006177</td>\n",
       "      <td>0.010482</td>\n",
       "      <td>0.001096</td>\n",
       "      <td>0.009684</td>\n",
       "      <td>0.005829</td>\n",
       "      <td>-0.010128</td>\n",
       "      <td>0.000880</td>\n",
       "      <td>-0.000569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.243905</td>\n",
       "      <td>-0.047781</td>\n",
       "      <td>-0.012217</td>\n",
       "      <td>-0.127877</td>\n",
       "      <td>-0.086277</td>\n",
       "      <td>0.078432</td>\n",
       "      <td>-0.099155</td>\n",
       "      <td>0.049713</td>\n",
       "      <td>-0.042611</td>\n",
       "      <td>0.048570</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025811</td>\n",
       "      <td>-0.011107</td>\n",
       "      <td>0.008496</td>\n",
       "      <td>-0.010481</td>\n",
       "      <td>-0.009029</td>\n",
       "      <td>-0.001121</td>\n",
       "      <td>-0.032290</td>\n",
       "      <td>0.022005</td>\n",
       "      <td>0.001010</td>\n",
       "      <td>-0.003441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.333545</td>\n",
       "      <td>-0.177561</td>\n",
       "      <td>-0.064060</td>\n",
       "      <td>0.134972</td>\n",
       "      <td>-0.003156</td>\n",
       "      <td>0.004314</td>\n",
       "      <td>-0.123386</td>\n",
       "      <td>-0.212981</td>\n",
       "      <td>0.079521</td>\n",
       "      <td>0.039691</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000126</td>\n",
       "      <td>0.004511</td>\n",
       "      <td>-0.003231</td>\n",
       "      <td>0.004600</td>\n",
       "      <td>0.006492</td>\n",
       "      <td>0.004050</td>\n",
       "      <td>-0.003285</td>\n",
       "      <td>-0.014750</td>\n",
       "      <td>0.010687</td>\n",
       "      <td>-0.008051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.328098</td>\n",
       "      <td>-0.016633</td>\n",
       "      <td>0.013271</td>\n",
       "      <td>-0.102437</td>\n",
       "      <td>0.034822</td>\n",
       "      <td>-0.105802</td>\n",
       "      <td>-0.053813</td>\n",
       "      <td>0.023793</td>\n",
       "      <td>0.013748</td>\n",
       "      <td>0.169843</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.023803</td>\n",
       "      <td>-0.042325</td>\n",
       "      <td>0.011858</td>\n",
       "      <td>-0.029245</td>\n",
       "      <td>0.033723</td>\n",
       "      <td>0.008265</td>\n",
       "      <td>-0.009333</td>\n",
       "      <td>0.007773</td>\n",
       "      <td>0.001593</td>\n",
       "      <td>0.030719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.219915</td>\n",
       "      <td>-0.069933</td>\n",
       "      <td>-0.018156</td>\n",
       "      <td>-0.056984</td>\n",
       "      <td>-0.085022</td>\n",
       "      <td>0.111803</td>\n",
       "      <td>-0.044039</td>\n",
       "      <td>-0.027794</td>\n",
       "      <td>-0.120207</td>\n",
       "      <td>0.011918</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011015</td>\n",
       "      <td>0.048171</td>\n",
       "      <td>-0.026972</td>\n",
       "      <td>-0.010325</td>\n",
       "      <td>-0.049737</td>\n",
       "      <td>-0.031136</td>\n",
       "      <td>-0.004143</td>\n",
       "      <td>0.044464</td>\n",
       "      <td>0.007657</td>\n",
       "      <td>0.054161</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 800 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0  0.138526 -0.019096 -0.009700 -0.091483 -0.041705  0.043097 -0.143737   \n",
       "1  0.243905 -0.047781 -0.012217 -0.127877 -0.086277  0.078432 -0.099155   \n",
       "2  0.333545 -0.177561 -0.064060  0.134972 -0.003156  0.004314 -0.123386   \n",
       "3  0.328098 -0.016633  0.013271 -0.102437  0.034822 -0.105802 -0.053813   \n",
       "4  0.219915 -0.069933 -0.018156 -0.056984 -0.085022  0.111803 -0.044039   \n",
       "\n",
       "        7         8         9      ...          790       791       792  \\\n",
       "0  0.060551 -0.029851  0.122997    ...    -0.000889  0.003325  0.006177   \n",
       "1  0.049713 -0.042611  0.048570    ...     0.025811 -0.011107  0.008496   \n",
       "2 -0.212981  0.079521  0.039691    ...     0.000126  0.004511 -0.003231   \n",
       "3  0.023793  0.013748  0.169843    ...    -0.023803 -0.042325  0.011858   \n",
       "4 -0.027794 -0.120207  0.011918    ...    -0.011015  0.048171 -0.026972   \n",
       "\n",
       "        793       794       795       796       797       798       799  \n",
       "0  0.010482  0.001096  0.009684  0.005829 -0.010128  0.000880 -0.000569  \n",
       "1 -0.010481 -0.009029 -0.001121 -0.032290  0.022005  0.001010 -0.003441  \n",
       "2  0.004600  0.006492  0.004050 -0.003285 -0.014750  0.010687 -0.008051  \n",
       "3 -0.029245  0.033723  0.008265 -0.009333  0.007773  0.001593  0.030719  \n",
       "4 -0.010325 -0.049737 -0.031136 -0.004143  0.044464  0.007657  0.054161  \n",
       "\n",
       "[5 rows x 800 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_df = pd.DataFrame(data=X_train_lsa)\n",
    "tfidf_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>796</th>\n",
       "      <th>797</th>\n",
       "      <th>798</th>\n",
       "      <th>799</th>\n",
       "      <th>word_count</th>\n",
       "      <th>exclamation_marks</th>\n",
       "      <th>periods</th>\n",
       "      <th>question_marks</th>\n",
       "      <th>Text</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.138526</td>\n",
       "      <td>-0.019096</td>\n",
       "      <td>-0.009700</td>\n",
       "      <td>-0.091483</td>\n",
       "      <td>-0.041705</td>\n",
       "      <td>0.043097</td>\n",
       "      <td>-0.143737</td>\n",
       "      <td>0.060551</td>\n",
       "      <td>-0.029851</td>\n",
       "      <td>0.122997</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005829</td>\n",
       "      <td>-0.010128</td>\n",
       "      <td>0.000880</td>\n",
       "      <td>-0.000569</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>i have bought several of the vitality canned d...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.243905</td>\n",
       "      <td>-0.047781</td>\n",
       "      <td>-0.012217</td>\n",
       "      <td>-0.127877</td>\n",
       "      <td>-0.086277</td>\n",
       "      <td>0.078432</td>\n",
       "      <td>-0.099155</td>\n",
       "      <td>0.049713</td>\n",
       "      <td>-0.042611</td>\n",
       "      <td>0.048570</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.032290</td>\n",
       "      <td>0.022005</td>\n",
       "      <td>0.001010</td>\n",
       "      <td>-0.003441</td>\n",
       "      <td>204</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>product arrived labeled as jumbo salted peanut...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.333545</td>\n",
       "      <td>-0.177561</td>\n",
       "      <td>-0.064060</td>\n",
       "      <td>0.134972</td>\n",
       "      <td>-0.003156</td>\n",
       "      <td>0.004314</td>\n",
       "      <td>-0.123386</td>\n",
       "      <td>-0.212981</td>\n",
       "      <td>0.079521</td>\n",
       "      <td>0.039691</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003285</td>\n",
       "      <td>-0.014750</td>\n",
       "      <td>0.010687</td>\n",
       "      <td>-0.008051</td>\n",
       "      <td>175</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>this is a confection that has been around a fe...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.328098</td>\n",
       "      <td>-0.016633</td>\n",
       "      <td>0.013271</td>\n",
       "      <td>-0.102437</td>\n",
       "      <td>0.034822</td>\n",
       "      <td>-0.105802</td>\n",
       "      <td>-0.053813</td>\n",
       "      <td>0.023793</td>\n",
       "      <td>0.013748</td>\n",
       "      <td>0.169843</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009333</td>\n",
       "      <td>0.007773</td>\n",
       "      <td>0.001593</td>\n",
       "      <td>0.030719</td>\n",
       "      <td>86</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>if you are looking for the secret ingredient i...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.219915</td>\n",
       "      <td>-0.069933</td>\n",
       "      <td>-0.018156</td>\n",
       "      <td>-0.056984</td>\n",
       "      <td>-0.085022</td>\n",
       "      <td>0.111803</td>\n",
       "      <td>-0.044039</td>\n",
       "      <td>-0.027794</td>\n",
       "      <td>-0.120207</td>\n",
       "      <td>0.011918</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004143</td>\n",
       "      <td>0.044464</td>\n",
       "      <td>0.007657</td>\n",
       "      <td>0.054161</td>\n",
       "      <td>60</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>great taffy at a great price. there was a wide...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 806 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  0.138526 -0.019096 -0.009700 -0.091483 -0.041705  0.043097 -0.143737   \n",
       "1  0.243905 -0.047781 -0.012217 -0.127877 -0.086277  0.078432 -0.099155   \n",
       "2  0.333545 -0.177561 -0.064060  0.134972 -0.003156  0.004314 -0.123386   \n",
       "3  0.328098 -0.016633  0.013271 -0.102437  0.034822 -0.105802 -0.053813   \n",
       "4  0.219915 -0.069933 -0.018156 -0.056984 -0.085022  0.111803 -0.044039   \n",
       "\n",
       "          7         8         9  ...         796       797       798  \\\n",
       "0  0.060551 -0.029851  0.122997  ...    0.005829 -0.010128  0.000880   \n",
       "1  0.049713 -0.042611  0.048570  ...   -0.032290  0.022005  0.001010   \n",
       "2 -0.212981  0.079521  0.039691  ...   -0.003285 -0.014750  0.010687   \n",
       "3  0.023793  0.013748  0.169843  ...   -0.009333  0.007773  0.001593   \n",
       "4 -0.027794 -0.120207  0.011918  ...   -0.004143  0.044464  0.007657   \n",
       "\n",
       "        799  word_count  exclamation_marks  periods  question_marks  \\\n",
       "0 -0.000569          64                0.0      3.0             0.0   \n",
       "1 -0.003441         204                0.0      5.0             0.0   \n",
       "2 -0.008051         175                0.0      9.0             0.0   \n",
       "3  0.030719          86                0.0      3.0             0.0   \n",
       "4  0.054161          60                0.0      4.0             0.0   \n",
       "\n",
       "                                                Text  Score  \n",
       "0  i have bought several of the vitality canned d...    5.0  \n",
       "1  product arrived labeled as jumbo salted peanut...    1.0  \n",
       "2  this is a confection that has been around a fe...    4.0  \n",
       "3  if you are looking for the secret ingredient i...    2.0  \n",
       "4  great taffy at a great price. there was a wide...    5.0  \n",
       "\n",
       "[5 rows x 806 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df = pd.concat([tfidf_df, feature_df], ignore_index=False, axis=1)\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = new_df.replace([np.inf, -np.inf], np.nan).dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare clustering variables  \n",
    "features = new_df.drop(['Score','Text'], axis=1)\n",
    "predict = new_df['Score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\shuaix\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\preprocessing\\data.py:323: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Clustering algorithms need normalization\n",
    "scalar = MinMaxScaler()\n",
    "\n",
    "scaled = scalar.fit_transform(features)\n",
    "scaled_df = pd.DataFrame(scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's see what the elbow curve tells us would be the optimal K value \n",
    "distortions = []\n",
    "K = range(1,10)\n",
    "for k in K:\n",
    "    kmeanModel = KMeans(n_clusters=k).fit(scaled)\n",
    "    kmeanModel.fit(scaled)\n",
    "    distortions.append(sum(np.min(cdist(scaled, kmeanModel.cluster_centers_, 'euclidean'), axis=1)) / scaled.shape[0])\n",
    "# Plot the elbow\n",
    "plt.plot(K, distortions, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Distortion')\n",
    "plt.title('Elbow Method for optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "looks like 2 clusters are best per the elbow method, perhaps due to sentiment in the reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Score</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>2764</td>\n",
       "      <td>832</td>\n",
       "      <td>474</td>\n",
       "      <td>676</td>\n",
       "      <td>5661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>1598</td>\n",
       "      <td>514</td>\n",
       "      <td>239</td>\n",
       "      <td>401</td>\n",
       "      <td>3304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>2398</td>\n",
       "      <td>688</td>\n",
       "      <td>364</td>\n",
       "      <td>599</td>\n",
       "      <td>4607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>4272</td>\n",
       "      <td>1317</td>\n",
       "      <td>681</td>\n",
       "      <td>1066</td>\n",
       "      <td>8977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.0</th>\n",
       "      <td>18773</td>\n",
       "      <td>5479</td>\n",
       "      <td>2982</td>\n",
       "      <td>4523</td>\n",
       "      <td>38715</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0      0     1     2     3      4\n",
       "Score                                \n",
       "1.0     2764   832   474   676   5661\n",
       "2.0     1598   514   239   401   3304\n",
       "3.0     2398   688   364   599   4607\n",
       "4.0     4272  1317   681  1066   8977\n",
       "5.0    18773  5479  2982  4523  38715"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Declare and fit clustering model using K of 5 for our 5 scores\n",
    "kmeans = KMeans(n_clusters=5, random_state=0)\n",
    "y_pred = kmeans.fit_predict(scaled)\n",
    "\n",
    "pd.crosstab(predict, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted Rand Score: 0.0026518\n",
      "Silhouette Score: -0.020762\n"
     ]
    }
   ],
   "source": [
    "print('Adjusted Rand Score: {:0.5}'.format(adjusted_rand_score(predict, y_pred)))\n",
    "print('Silhouette Score: {:0.5}'.format(silhouette_score(scaled, y_pred, metric='euclidean')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minibatch KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing k-means and mini batch k-means solutions:\n",
      "col_0      0      1     2      3  4\n",
      "Score                              \n",
      "1.0     3973   4255   697   1482  0\n",
      "2.0     2283   2469   378    925  1\n",
      "3.0     3267   3589   566   1234  0\n",
      "4.0     6203   6580  1080   2450  0\n",
      "5.0    26976  28515  4741  10240  0\n"
     ]
    }
   ],
   "source": [
    "# Each batch will be made up of 200 data points.\n",
    "minibatchkmeans = MiniBatchKMeans(\n",
    "    init='k-means++',\n",
    "    n_clusters=5,\n",
    "    batch_size=200)\n",
    "minibatchkmeans.fit(scaled)\n",
    "\n",
    "# Add the new predicted cluster memberships to the data frame.\n",
    "predict_mini = minibatchkmeans.predict(scaled)\n",
    "\n",
    "# Check the MiniBatch model against our earlier one.\n",
    "print('Comparing k-means and mini batch k-means solutions:')\n",
    "print(pd.crosstab(predict, predict_mini))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted Rand Score: -0.00024463\n",
      "Silhouette Score: -0.011988\n"
     ]
    }
   ],
   "source": [
    "print('Adjusted Rand Score: {:0.5}'.format(adjusted_rand_score(predict, predict_mini)))\n",
    "print('Silhouette Score: {:0.5}'.format(silhouette_score(scaled, predict_mini, metric='euclidean')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From what we can see with these two clustering methods scores being close to zero, it appears that most of the observations lie between two clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling: Training Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression: Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\shuaix\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\shuaix\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "c:\\users\\shuaix\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\shuaix\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "c:\\users\\shuaix\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\shuaix\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "c:\\users\\shuaix\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\shuaix\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "c:\\users\\shuaix\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\shuaix\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "c:\\users\\shuaix\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\shuaix\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cross Validation Training Scores:0.58371(+/- 0.003)\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(class_weight = 'balanced')\n",
    "train = lr.fit(scaled, predict)\n",
    "lr_scores = cross_val_score(lr, scaled, predict, cv=5)\n",
    "\n",
    "print('\\nCross Validation Training Scores:{:.5f}(+/- {:.3f})'.format(lr_scores.mean(), lr_scores.std()*2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Clasification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         1.0   0.228627  0.056789  0.090979     10407\n",
      "         2.0   0.137237  0.075462  0.097379      6056\n",
      "         3.0   0.178031  0.072089  0.102623      8656\n",
      "         4.0   0.266446  0.034512  0.061109     16313\n",
      "         5.0   0.650507  0.926496  0.764351     70472\n",
      "\n",
      "   micro avg   0.603437  0.603437  0.603437    111904\n",
      "   macro avg   0.292170  0.233070  0.223288    111904\n",
      "weighted avg   0.490961  0.603437  0.511930    111904\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[  591   392   432   233  8759]\n",
      " [  212   457   249   135  5003]\n",
      " [  278   352   624   227  7175]\n",
      " [  403   588   617   563 14142]\n",
      " [ 1101  1541  1583   955 65292]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "sk_lr_report = classification_report(\n",
    "    digits=6,\n",
    "    y_true=predict, \n",
    "    y_pred=lr.predict(scaled))\n",
    "print('\\nClasification report:\\n', sk_lr_report)\n",
    "\n",
    "sk_lr_report2 = confusion_matrix(y_true=predict, y_pred=lr.predict(scaled))\n",
    "\n",
    "print('\\nConfusion Matrix:\\n',sk_lr_report2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest: Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\shuaix\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cross Validation Training Scores:0.60154(+/- 0.001)\n"
     ]
    }
   ],
   "source": [
    "rfc = RandomForestClassifier(class_weight = 'balanced_subsample')\n",
    "train = rfc.fit(scaled, predict)\n",
    "rfc_scores = cross_val_score(rfc, scaled, predict, cv=5)\n",
    "\n",
    "print('\\nCross Validation Training Scores:{:.5f}(+/- {:.3f})'.format(rfc_scores.mean(), rfc_scores.std()*2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Clasification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         1.0   0.998207  0.962717  0.980141     10407\n",
      "         2.0   0.999484  0.959379  0.979021      6056\n",
      "         3.0   0.999041  0.962338  0.980346      8656\n",
      "         4.0   0.998165  0.966959  0.982314     16313\n",
      "         5.0   0.979642  0.999674  0.989556     70472\n",
      "\n",
      "   micro avg   0.986399  0.986399  0.986399    111904\n",
      "   macro avg   0.994908  0.970213  0.982276    111904\n",
      "weighted avg   0.986643  0.986399  0.986342    111904\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[10019     1     1     5   381]\n",
      " [    5  5810     2     3   236]\n",
      " [    3     0  8330     5   318]\n",
      " [    8     0     2 15774   529]\n",
      " [    2     2     3    16 70449]]\n"
     ]
    }
   ],
   "source": [
    "sk_rfc_report = classification_report(\n",
    "    digits=6,\n",
    "    y_true=predict, \n",
    "    y_pred=rfc.predict(scaled))\n",
    "print('\\nClasification report:\\n', sk_rfc_report)\n",
    "\n",
    "sk_rfc_report2 = confusion_matrix(y_true=predict, y_pred=rfc.predict(scaled))\n",
    "\n",
    "print('\\nConfusion Matrix:\\n',sk_rfc_report2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling: Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need to process the test group as we did the training set above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total word count \n",
    "feature_df_test = pd.DataFrame()\n",
    "feature_df_test['word_count'] = [len(x.split()) for x in X_test.tolist()]\n",
    "\n",
    "# Count of punctuations \n",
    "feature_df_test['exclamation_marks'] = X_test.str.findall(r'[!]').str.len()\n",
    "feature_df_test['periods'] = X_test.str.findall(r'[.]').str.len()\n",
    "feature_df_test['question_marks'] = X_test.str.findall(r'[?]').str.len()\n",
    "feature_df_test['Text'] = X_test\n",
    "feature_df_test['Score'] = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent variance captured by all components: 54.64595748970055\n"
     ]
    }
   ],
   "source": [
    "#Our SVD data reducer.  We are going to reduce the feature space to 800 again for test.\n",
    "svd= TruncatedSVD(800)\n",
    "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "# Run SVD on the training data, then project the training data.\n",
    "X_test_lsa = lsa.fit_transform(X_test_tfidf)\n",
    "\n",
    "variance_explained=svd.explained_variance_ratio_\n",
    "total_variance = variance_explained.sum()\n",
    "\n",
    "print(\"Percent variance captured by all components:\",total_variance*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>790</th>\n",
       "      <th>791</th>\n",
       "      <th>792</th>\n",
       "      <th>793</th>\n",
       "      <th>794</th>\n",
       "      <th>795</th>\n",
       "      <th>796</th>\n",
       "      <th>797</th>\n",
       "      <th>798</th>\n",
       "      <th>799</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.149010</td>\n",
       "      <td>-0.056183</td>\n",
       "      <td>-0.001003</td>\n",
       "      <td>-0.101753</td>\n",
       "      <td>-0.100769</td>\n",
       "      <td>-0.030476</td>\n",
       "      <td>0.090389</td>\n",
       "      <td>0.064680</td>\n",
       "      <td>-0.012433</td>\n",
       "      <td>-0.076608</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019578</td>\n",
       "      <td>-0.012207</td>\n",
       "      <td>0.026662</td>\n",
       "      <td>0.043037</td>\n",
       "      <td>-0.038824</td>\n",
       "      <td>0.005555</td>\n",
       "      <td>0.030779</td>\n",
       "      <td>-0.004697</td>\n",
       "      <td>0.000172</td>\n",
       "      <td>-0.010152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.224022</td>\n",
       "      <td>-0.158376</td>\n",
       "      <td>0.001396</td>\n",
       "      <td>0.211602</td>\n",
       "      <td>-0.155638</td>\n",
       "      <td>-0.016718</td>\n",
       "      <td>-0.070060</td>\n",
       "      <td>-0.003110</td>\n",
       "      <td>-0.098705</td>\n",
       "      <td>-0.003249</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000253</td>\n",
       "      <td>0.005644</td>\n",
       "      <td>-0.005310</td>\n",
       "      <td>0.009057</td>\n",
       "      <td>-0.004556</td>\n",
       "      <td>-0.007875</td>\n",
       "      <td>0.008396</td>\n",
       "      <td>-0.013870</td>\n",
       "      <td>0.002466</td>\n",
       "      <td>-0.006937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.217265</td>\n",
       "      <td>-0.103008</td>\n",
       "      <td>-0.007078</td>\n",
       "      <td>-0.037072</td>\n",
       "      <td>0.058375</td>\n",
       "      <td>-0.042559</td>\n",
       "      <td>-0.024804</td>\n",
       "      <td>0.068597</td>\n",
       "      <td>0.059574</td>\n",
       "      <td>0.023518</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025184</td>\n",
       "      <td>0.002302</td>\n",
       "      <td>0.001509</td>\n",
       "      <td>0.006538</td>\n",
       "      <td>0.028943</td>\n",
       "      <td>-0.014825</td>\n",
       "      <td>-0.006719</td>\n",
       "      <td>-0.009710</td>\n",
       "      <td>0.005816</td>\n",
       "      <td>-0.011967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.210651</td>\n",
       "      <td>0.009628</td>\n",
       "      <td>0.009356</td>\n",
       "      <td>-0.113185</td>\n",
       "      <td>-0.149731</td>\n",
       "      <td>0.042603</td>\n",
       "      <td>-0.054715</td>\n",
       "      <td>-0.049735</td>\n",
       "      <td>-0.079807</td>\n",
       "      <td>0.012954</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.013249</td>\n",
       "      <td>0.012708</td>\n",
       "      <td>0.002658</td>\n",
       "      <td>0.005951</td>\n",
       "      <td>-0.023002</td>\n",
       "      <td>0.004106</td>\n",
       "      <td>-0.013160</td>\n",
       "      <td>0.008318</td>\n",
       "      <td>-0.002015</td>\n",
       "      <td>-0.006701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.165193</td>\n",
       "      <td>-0.078425</td>\n",
       "      <td>0.007443</td>\n",
       "      <td>0.044326</td>\n",
       "      <td>0.110497</td>\n",
       "      <td>0.027904</td>\n",
       "      <td>-0.029237</td>\n",
       "      <td>0.020718</td>\n",
       "      <td>-0.053072</td>\n",
       "      <td>-0.016590</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011991</td>\n",
       "      <td>-0.011280</td>\n",
       "      <td>0.006777</td>\n",
       "      <td>0.005868</td>\n",
       "      <td>-0.012937</td>\n",
       "      <td>0.012778</td>\n",
       "      <td>-0.002174</td>\n",
       "      <td>0.011590</td>\n",
       "      <td>0.018691</td>\n",
       "      <td>-0.003250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 800 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0  0.149010 -0.056183 -0.001003 -0.101753 -0.100769 -0.030476  0.090389   \n",
       "1  0.224022 -0.158376  0.001396  0.211602 -0.155638 -0.016718 -0.070060   \n",
       "2  0.217265 -0.103008 -0.007078 -0.037072  0.058375 -0.042559 -0.024804   \n",
       "3  0.210651  0.009628  0.009356 -0.113185 -0.149731  0.042603 -0.054715   \n",
       "4  0.165193 -0.078425  0.007443  0.044326  0.110497  0.027904 -0.029237   \n",
       "\n",
       "        7         8         9      ...          790       791       792  \\\n",
       "0  0.064680 -0.012433 -0.076608    ...     0.019578 -0.012207  0.026662   \n",
       "1 -0.003110 -0.098705 -0.003249    ...    -0.000253  0.005644 -0.005310   \n",
       "2  0.068597  0.059574  0.023518    ...     0.025184  0.002302  0.001509   \n",
       "3 -0.049735 -0.079807  0.012954    ...    -0.013249  0.012708  0.002658   \n",
       "4  0.020718 -0.053072 -0.016590    ...     0.011991 -0.011280  0.006777   \n",
       "\n",
       "        793       794       795       796       797       798       799  \n",
       "0  0.043037 -0.038824  0.005555  0.030779 -0.004697  0.000172 -0.010152  \n",
       "1  0.009057 -0.004556 -0.007875  0.008396 -0.013870  0.002466 -0.006937  \n",
       "2  0.006538  0.028943 -0.014825 -0.006719 -0.009710  0.005816 -0.011967  \n",
       "3  0.005951 -0.023002  0.004106 -0.013160  0.008318 -0.002015 -0.006701  \n",
       "4  0.005868 -0.012937  0.012778 -0.002174  0.011590  0.018691 -0.003250  \n",
       "\n",
       "[5 rows x 800 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_df_test = pd.DataFrame(data=X_test_lsa)\n",
    "tfidf_df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>796</th>\n",
       "      <th>797</th>\n",
       "      <th>798</th>\n",
       "      <th>799</th>\n",
       "      <th>word_count</th>\n",
       "      <th>exclamation_marks</th>\n",
       "      <th>periods</th>\n",
       "      <th>question_marks</th>\n",
       "      <th>Text</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.149010</td>\n",
       "      <td>-0.056183</td>\n",
       "      <td>-0.001003</td>\n",
       "      <td>-0.101753</td>\n",
       "      <td>-0.100769</td>\n",
       "      <td>-0.030476</td>\n",
       "      <td>0.090389</td>\n",
       "      <td>0.064680</td>\n",
       "      <td>-0.012433</td>\n",
       "      <td>-0.076608</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030779</td>\n",
       "      <td>-0.004697</td>\n",
       "      <td>0.000172</td>\n",
       "      <td>-0.010152</td>\n",
       "      <td>65</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.224022</td>\n",
       "      <td>-0.158376</td>\n",
       "      <td>0.001396</td>\n",
       "      <td>0.211602</td>\n",
       "      <td>-0.155638</td>\n",
       "      <td>-0.016718</td>\n",
       "      <td>-0.070060</td>\n",
       "      <td>-0.003110</td>\n",
       "      <td>-0.098705</td>\n",
       "      <td>-0.003249</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008396</td>\n",
       "      <td>-0.013870</td>\n",
       "      <td>0.002466</td>\n",
       "      <td>-0.006937</td>\n",
       "      <td>30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.217265</td>\n",
       "      <td>-0.103008</td>\n",
       "      <td>-0.007078</td>\n",
       "      <td>-0.037072</td>\n",
       "      <td>0.058375</td>\n",
       "      <td>-0.042559</td>\n",
       "      <td>-0.024804</td>\n",
       "      <td>0.068597</td>\n",
       "      <td>0.059574</td>\n",
       "      <td>0.023518</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006719</td>\n",
       "      <td>-0.009710</td>\n",
       "      <td>0.005816</td>\n",
       "      <td>-0.011967</td>\n",
       "      <td>58</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.210651</td>\n",
       "      <td>0.009628</td>\n",
       "      <td>0.009356</td>\n",
       "      <td>-0.113185</td>\n",
       "      <td>-0.149731</td>\n",
       "      <td>0.042603</td>\n",
       "      <td>-0.054715</td>\n",
       "      <td>-0.049735</td>\n",
       "      <td>-0.079807</td>\n",
       "      <td>0.012954</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.013160</td>\n",
       "      <td>0.008318</td>\n",
       "      <td>-0.002015</td>\n",
       "      <td>-0.006701</td>\n",
       "      <td>115</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.165193</td>\n",
       "      <td>-0.078425</td>\n",
       "      <td>0.007443</td>\n",
       "      <td>0.044326</td>\n",
       "      <td>0.110497</td>\n",
       "      <td>0.027904</td>\n",
       "      <td>-0.029237</td>\n",
       "      <td>0.020718</td>\n",
       "      <td>-0.053072</td>\n",
       "      <td>-0.016590</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002174</td>\n",
       "      <td>0.011590</td>\n",
       "      <td>0.018691</td>\n",
       "      <td>-0.003250</td>\n",
       "      <td>76</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 806 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  0.149010 -0.056183 -0.001003 -0.101753 -0.100769 -0.030476  0.090389   \n",
       "1  0.224022 -0.158376  0.001396  0.211602 -0.155638 -0.016718 -0.070060   \n",
       "2  0.217265 -0.103008 -0.007078 -0.037072  0.058375 -0.042559 -0.024804   \n",
       "3  0.210651  0.009628  0.009356 -0.113185 -0.149731  0.042603 -0.054715   \n",
       "4  0.165193 -0.078425  0.007443  0.044326  0.110497  0.027904 -0.029237   \n",
       "\n",
       "          7         8         9  ...         796       797       798  \\\n",
       "0  0.064680 -0.012433 -0.076608  ...    0.030779 -0.004697  0.000172   \n",
       "1 -0.003110 -0.098705 -0.003249  ...    0.008396 -0.013870  0.002466   \n",
       "2  0.068597  0.059574  0.023518  ...   -0.006719 -0.009710  0.005816   \n",
       "3 -0.049735 -0.079807  0.012954  ...   -0.013160  0.008318 -0.002015   \n",
       "4  0.020718 -0.053072 -0.016590  ...   -0.002174  0.011590  0.018691   \n",
       "\n",
       "        799  word_count  exclamation_marks  periods  question_marks  Text  \\\n",
       "0 -0.010152          65                NaN      NaN             NaN   NaN   \n",
       "1 -0.006937          30                NaN      NaN             NaN   NaN   \n",
       "2 -0.011967          58                NaN      NaN             NaN   NaN   \n",
       "3 -0.006701         115                NaN      NaN             NaN   NaN   \n",
       "4 -0.003250          76                NaN      NaN             NaN   NaN   \n",
       "\n",
       "   Score  \n",
       "0    NaN  \n",
       "1    NaN  \n",
       "2    NaN  \n",
       "3    NaN  \n",
       "4    NaN  \n",
       "\n",
       "[5 rows x 806 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df = pd.concat([tfidf_df_test, feature_df_test], ignore_index=False, axis=1)\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = new_df.replace([np.inf, -np.inf], np.nan).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\shuaix\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\preprocessing\\data.py:323: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    }
   ],
   "source": [
    "# Declare variables  \n",
    "test_features = new_df.drop(['Score','Text'], axis=1)\n",
    "test_predict = new_df['Score']\n",
    "\n",
    "# Normalize data \n",
    "scalar = MinMaxScaler()\n",
    "\n",
    "test_scaled = scalar.fit_transform(test_features)\n",
    "test_df = pd.DataFrame(test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Score</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>262</td>\n",
       "      <td>2410</td>\n",
       "      <td>674</td>\n",
       "      <td>1293</td>\n",
       "      <td>213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>147</td>\n",
       "      <td>1453</td>\n",
       "      <td>355</td>\n",
       "      <td>760</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>229</td>\n",
       "      <td>2172</td>\n",
       "      <td>551</td>\n",
       "      <td>1119</td>\n",
       "      <td>188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>433</td>\n",
       "      <td>3875</td>\n",
       "      <td>1046</td>\n",
       "      <td>2061</td>\n",
       "      <td>345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.0</th>\n",
       "      <td>1761</td>\n",
       "      <td>16933</td>\n",
       "      <td>4495</td>\n",
       "      <td>9175</td>\n",
       "      <td>1399</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0     0      1     2     3     4\n",
       "Score                               \n",
       "1.0     262   2410   674  1293   213\n",
       "2.0     147   1453   355   760   128\n",
       "3.0     229   2172   551  1119   188\n",
       "4.0     433   3875  1046  2061   345\n",
       "5.0    1761  16933  4495  9175  1399"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Declare and fit clustering model using K of 5 for our 5 scores\n",
    "kmeans = KMeans(n_clusters=5, random_state=0)\n",
    "y_pred = kmeans.fit_predict(test_scaled)\n",
    "\n",
    "pd.crosstab(test_predict, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted Rand Score: 0.00071531\n"
     ]
    }
   ],
   "source": [
    "print('Adjusted Rand Score: {:0.5}'.format(adjusted_rand_score(test_predict, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minibatch KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing k-means and mini batch k-means solutions:\n",
      "col_0    0      1      2     3     4\n",
      "Score                               \n",
      "1.0     22   1821   2206   253   550\n",
      "2.0     24   1104   1259   157   299\n",
      "3.0     31   1612   1903   217   496\n",
      "4.0     48   2923   3498   426   865\n",
      "5.0    231  12581  15388  1756  3807\n"
     ]
    }
   ],
   "source": [
    "# Each batch will be made up of 200 data points.\n",
    "minibatchkmeans = MiniBatchKMeans(\n",
    "    init='k-means++',\n",
    "    n_clusters=5,\n",
    "    batch_size=200)\n",
    "minibatchkmeans.fit(test_scaled)\n",
    "\n",
    "# Add the new predicted cluster memberships to the data frame.\n",
    "predict_mini = minibatchkmeans.predict(test_scaled)\n",
    "\n",
    "# Check the MiniBatch model against our earlier one.\n",
    "print('Comparing k-means and mini batch k-means solutions:')\n",
    "print(pd.crosstab(test_predict, predict_mini))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted Rand Score: 0.00057498\n"
     ]
    }
   ],
   "source": [
    "print('Adjusted Rand Score: {:0.5}'.format(adjusted_rand_score(test_predict, predict_mini)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression: Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Test Score: 0.33665\n"
     ]
    }
   ],
   "source": [
    "print('Logistic Regression Test Score: {:0.5}'.format(lr.score(test_scaled, test_predict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Clasification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         1.0   0.200000  0.001443  0.002865      4852\n",
      "         2.0   0.066401  0.720366  0.121594      2843\n",
      "         3.0   0.076142  0.003522  0.006732      4259\n",
      "         4.0   0.147975  0.012242  0.022614      7760\n",
      "         5.0   0.727849  0.469093  0.570502     33763\n",
      "\n",
      "   micro avg   0.336649  0.336649  0.336649     53477\n",
      "   macro avg   0.243673  0.241333  0.144861     53477\n",
      "weighted avg   0.508744  0.336649  0.370732     53477\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[    7  3119    26    57  1643]\n",
      " [    4  2048    22    39   730]\n",
      " [    4  3148    15    70  1022]\n",
      " [    5  5092    41    95  2527]\n",
      " [   15 17436    93   381 15838]]\n"
     ]
    }
   ],
   "source": [
    "sk_lr_report = classification_report(\n",
    "    digits=6,\n",
    "    y_true=test_predict, \n",
    "    y_pred=lr.predict(test_scaled))\n",
    "print('\\nClasification report:\\n', sk_lr_report)\n",
    "\n",
    "sk_lr_report2 = confusion_matrix(y_true=test_predict, y_pred=lr.predict(test_scaled))\n",
    "\n",
    "print('\\nConfusion Matrix:\\n',sk_lr_report2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest: Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Test Score: 0.59543\n"
     ]
    }
   ],
   "source": [
    "print('Random Forest Test Score: {:0.5}'.format(rfc.score(test_scaled, test_predict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Clasification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         1.0   0.092531  0.017106  0.028875      4852\n",
      "         2.0   0.073434  0.011959  0.020569      2843\n",
      "         3.0   0.097601  0.027706  0.043160      4259\n",
      "         4.0   0.141176  0.023196  0.039845      7760\n",
      "         5.0   0.633188  0.930812  0.753681     33763\n",
      "\n",
      "   micro avg   0.595434  0.595434  0.595434     53477\n",
      "   macro avg   0.207586  0.202156  0.177226     53477\n",
      "weighted avg   0.440325  0.595434  0.488773     53477\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[   83    42   115   124  4488]\n",
      " [   59    34    72    70  2608]\n",
      " [   97    43   118   108  3893]\n",
      " [  110    74   179   180  7217]\n",
      " [  548   270   725   793 31427]]\n"
     ]
    }
   ],
   "source": [
    "sk_rfc_report = classification_report(\n",
    "    digits=6,\n",
    "    y_true=test_predict, \n",
    "    y_pred=rfc.predict(test_scaled))\n",
    "print('\\nClasification report:\\n', sk_rfc_report)\n",
    "\n",
    "sk_rfc_report2 = confusion_matrix(y_true=test_predict, y_pred=rfc.predict(test_scaled))\n",
    "\n",
    "print('\\nConfusion Matrix:\\n',sk_rfc_report2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modeling versus Clustering?\n",
    "\n",
    "Overall, our supervised models were much better at classifying reviews than clustering. Though our models did tend to overfit on the test set.\n",
    "\n",
    "Unfortunately we weren't able to test many different clustering methods given the size of the dataset and computational limitations, and thus our Kmeans cluster was effectively the 'best' given its ability to actualy execute on the dataset.\n",
    "\n",
    "Ideally, we would be able to keep around 90% of the variance in our truncated SVD, but due to computational limitations, I was only able to perform clustering and modeling with 50% of the variance kept. Without this limitation, we would likely see an increase in our clustering and modeling methods.\n",
    "\n",
    "In the future, I would like to fine tune parameters on our clusters and models, increase our variance incorporated on the tfidf vectorizer to see if we can better our overall performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
